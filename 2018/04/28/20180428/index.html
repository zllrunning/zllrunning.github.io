<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="zll">
  <!-- Open Graph Data -->
  <meta property="og:title" content="Pytorch模型可视化"/>
  <meta property="og:description" content="石头的记录" />
  <meta property="og:site_name" content="小小石头"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://zllrunning.github.io"/>
  
    <link rel="alternate" href="/atom.xml" title="小小石头" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  

  <!-- Site Title -->
  <title>小小石头</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">Pytorch模型可视化</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/zllrunning">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:zllrunning@gmail.com">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By zll</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2018-04-28</span>
            <span class="time">13:30:45</span>
          </span>

          
          <!--  Categories  -->
            <span class="categories info">Under 

<a href="/categories/深度学习/">深度学习</a>
</span>
          
        </div>
        <!-- Tags -->
        
          <div class="post-tags text-muted">
            Tags: 

<a class="tag" href="/tags/Pytorch/">#Pytorch</a>


	  <span id="busuanzi_container_page_pv">
   		本文浏览量<span id="busuanzi_value_page_pv"></span>次
	  </span>
          </div>
        
        <!-- Post Main Content -->
        <div class="post-content">
          <p>不知道你有没有查看模型各层输出尺寸的需要，今天介绍一个简单的库来达到这个目的。<br><a id="more"></a></p>
<p>##torchsummary<br>torchsummary这个工具可以用于模型的可视化，会输出模型各层的详细参数以及各层输出的尺寸。有的时候我们还是很在乎每层输出的shape的，用这个就比较容易查看了，用起来很方便。</p>
<p>##torchsummary的安装<br><code>pip install torchsummary</code></p>
<p>##torchsummary的使用</p>
<p>####torchsummary使用起来很简单，直接调用<code>summary()</code>，传入你的model以及model输入tensor的尺寸就可以了，因为在这里是做了一个前向的运算的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from torchsummary import summary</span><br><span class="line">summary(your_model, input_size=(channels, H, W))</span><br></pre></td></tr></table></figure></p>
<p>##小例子</p>
<p>##<code>torchvision.models</code>的model<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchsummary import summary</span><br><span class="line">from torchvision.models import vgg11</span><br><span class="line"></span><br><span class="line">model = vgg11(pretrained=False)</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    model.cuda()</span><br><span class="line">summary(model, (3, 224, 224))</span><br></pre></td></tr></table></figure></p>
<p><strong>输出</strong><br>这里的output shape就是每层输出的tensor的shape<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">================================================================</span><br><span class="line">            Conv2d-1         [-1, 64, 224, 224]            1792</span><br><span class="line">              ReLU-2         [-1, 64, 224, 224]               0</span><br><span class="line">         MaxPool2d-3         [-1, 64, 112, 112]               0</span><br><span class="line">            Conv2d-4        [-1, 128, 112, 112]           73856</span><br><span class="line">              ReLU-5        [-1, 128, 112, 112]               0</span><br><span class="line">         MaxPool2d-6          [-1, 128, 56, 56]               0</span><br><span class="line">            Conv2d-7          [-1, 256, 56, 56]          295168</span><br><span class="line">              ReLU-8          [-1, 256, 56, 56]               0</span><br><span class="line">            Conv2d-9          [-1, 256, 56, 56]          590080</span><br><span class="line">             ReLU-10          [-1, 256, 56, 56]               0</span><br><span class="line">        MaxPool2d-11          [-1, 256, 28, 28]               0</span><br><span class="line">           Conv2d-12          [-1, 512, 28, 28]         1180160</span><br><span class="line">             ReLU-13          [-1, 512, 28, 28]               0</span><br><span class="line">           Conv2d-14          [-1, 512, 28, 28]         2359808</span><br><span class="line">             ReLU-15          [-1, 512, 28, 28]               0</span><br><span class="line">        MaxPool2d-16          [-1, 512, 14, 14]               0</span><br><span class="line">           Conv2d-17          [-1, 512, 14, 14]         2359808</span><br><span class="line">             ReLU-18          [-1, 512, 14, 14]               0</span><br><span class="line">           Conv2d-19          [-1, 512, 14, 14]         2359808</span><br><span class="line">             ReLU-20          [-1, 512, 14, 14]               0</span><br><span class="line">        MaxPool2d-21            [-1, 512, 7, 7]               0</span><br><span class="line">           Linear-22                 [-1, 4096]       102764544</span><br><span class="line">             ReLU-23                 [-1, 4096]               0</span><br><span class="line">          Dropout-24                 [-1, 4096]               0</span><br><span class="line">           Linear-25                 [-1, 4096]        16781312</span><br><span class="line">             ReLU-26                 [-1, 4096]               0</span><br><span class="line">          Dropout-27                 [-1, 4096]               0</span><br><span class="line">           Linear-28                 [-1, 1000]         4097000</span><br><span class="line">================================================================</span><br><span class="line">Total params: 132863336</span><br><span class="line">Trainable params: 132863336</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure></p>
<p>###小例子2<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model.cuda()</span><br><span class="line"></span><br><span class="line">summary(model, (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br></pre></td></tr></table></figure></p>
<p><strong>输出</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">================================================================</span><br><span class="line">            Conv2d-1           [-1, 10, 24, 24]             260</span><br><span class="line">            Conv2d-2             [-1, 20, 8, 8]            5020</span><br><span class="line">         Dropout2d-3             [-1, 20, 8, 8]               0</span><br><span class="line">            Linear-4                   [-1, 50]           16050</span><br><span class="line">            Linear-5                   [-1, 10]             510</span><br><span class="line">================================================================</span><br><span class="line">Total params: 21840</span><br><span class="line">Trainable params: 21840</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure></p>
<p>###地址在这里<br><code>https://github.com/sksq96/pytorch-summary</code></p>

        </div>
<div id="gitmentContainer"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
    owner: 'zllrunning',
    repo: 'zllrunning.github.io',
    oauth: {
        client_id: 'e6f46c4e8b4073d9bc67',
        client_secret: '3d4cbc0a4cd9abd0ca6d6a691e20f9161a11677f',
    },
});
gitment.render('gitmentContainer');
</script>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5kc74bk53wq&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
	   <span id="busuanzi_container_site_pv">
    	     本站总访问量<span id="busuanzi_value_site_pv"></span>次
	   </span>
        </p>
      </div>
    </div>
  </div>
</footer>



    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

